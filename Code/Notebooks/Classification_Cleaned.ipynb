{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6adb6a77",
   "metadata": {},
   "source": [
    "# Ticket Classification — Supervised Models\n",
    "This notebook demonstrates how to classify support tickets using traditional and transformer-based models.\n",
    "\n",
    "### Available Options:\n",
    "- Logistic Regression with TF-IDF\n",
    "- XLM-RoBERTa (via Hugging Face Transformers)\n",
    "\n",
    "_All input goes through privacy filtering and optional augmentation._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f88ff6",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Install Dependencies (if not already installed) ===\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn plotly wordcloud torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08624393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Purpose ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Visualization ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# === Scikit-learn ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve, calibration_curve\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# === Hugging Face Transformers ===\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# === Miscellaneous ===\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3ac39",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "TRAIN_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 256\n",
    "FAST_RUN = False\n",
    "DATA_PATH = \"cleaned.csv\"\n",
    "USE_TFIDF_MODEL = True  # Set False to skip logistic regression\n",
    "\n",
    "# Model config tweaks\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.hidden_dropout_prob = 0.3\n",
    "config.attention_probs_dropout_prob = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6eef2c",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# (Ensure this is privacy-filtered and optionally augmented)\n",
    "df = pd.read_csv(DATA_PATH, sep=',')\n",
    "\n",
    "X = df[\"combined_text\"].astype(str)\n",
    "y = df[\"Issue Type\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split (for Augmented Datasets)\n",
    "\n",
    "If your dataset already includes a `split` column (e.g., from backtranslation or manual splitting), use the code below to separate training and testing data accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on pre-defined 'split' labels\n",
    "train_df = df[df['split'] == 'train']\n",
    "test_df = df[df['split'] == 'test']\n",
    "\n",
    "X_train = train_df[\"combined_text\"].astype(str)\n",
    "y_train = train_df[\"Issue Type\"]\n",
    "\n",
    "X_test = test_df[\"combined_text\"].astype(str)\n",
    "y_test = test_df[\"Issue Type\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Shape (Optional)\n",
    "\n",
    "This creates a circular mask for the word cloud. You can later replace it with a custom image if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple circular mask for word cloud\n",
    "h, w = 400, 400\n",
    "sy, sx = np.ogrid[:h, :w]\n",
    "center_x, center_y = w // 2, h // 2\n",
    "radius = w // 2\n",
    "\n",
    "circle = (sx - center_x) ** 2 + (sy - center_y) ** 2 > radius ** 2\n",
    "mask = np.zeros((h, w), dtype=np.uint8)\n",
    "mask[circle] = 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution Check\n",
    "\n",
    "Quickly verify how labels are distributed across the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count labels in each split\n",
    "train_label_counts = y_train.value_counts()\n",
    "test_label_counts = y_test.value_counts()\n",
    "\n",
    "print(\"Training Set Label Counts:\")\n",
    "print(train_label_counts)\n",
    "\n",
    "print(\"\\nTest Set Label Counts:\")\n",
    "print(test_label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + Logistic Regression Baseline\n",
    "\n",
    "This baseline uses a traditional machine learning pipeline:\n",
    "\n",
    "- **TF-IDF vectorization** on Dutch ticket text (with unigrams and bigrams)\n",
    "- **Logistic Regression** with class weighting to handle label imbalance\n",
    "\n",
    "Use this as a benchmark for comparing with transformer-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TFIDF_MODEL:\n",
    "    print(\"Training TF-IDF + Logistic Regression baseline...\")\n",
    "\n",
    "    # Vectorize input text using unigrams + bigrams\n",
    "    tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    # Fit logistic regression with class balancing\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        class_weight='balanced',\n",
    "        C=4\n",
    "    )\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "\n",
    "    print(\"\\nTF-IDF Model Evaluation:\")\n",
    "    print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(\n",
    "        confusion_matrix(y_test, y_pred_tfidf),\n",
    "        annot=True, fmt='d', cmap='Blues'\n",
    "    )\n",
    "    plt.title(\"TF-IDF + Logistic Regression Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model Visualizations\n",
    "\n",
    "This section provides a deeper analysis of the TF-IDF + Logistic Regression model:\n",
    "\n",
    "- ROC and Precision-Recall curves (multiclass)\n",
    "- Class-specific feature importance (top TF-IDF coefficients)\n",
    "- Word clouds per class\n",
    "- Calibration curves to evaluate probability reliability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Precision-Recall Curves (TF-IDF Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize y for multiclass evaluation\n",
    "classes = clf.classes_\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "y_score = clf.decision_function(X_test_tfidf)\n",
    "\n",
    "# === Multiclass ROC ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, cls in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr, tpr):.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multiclass ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Multiclass Precision-Recall ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, cls in enumerate(classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, label=f\"{cls} (AP={auc(recall, precision):.2f})\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Multiclass Precision-Recall Curves')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance and Word Clouds (TF-IDF Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize most influential TF-IDF features per class\n",
    "features = tfidf.get_feature_names_out()\n",
    "coefs = clf.coef_\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    imp_df = pd.DataFrame({'feature': features, 'coef': coefs[i]})\n",
    "    top_pos = imp_df.nlargest(10, 'coef')\n",
    "    top_neg = imp_df.nsmallest(10, 'coef')\n",
    "\n",
    "    # === Barplot ===\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        data=pd.concat([top_pos, top_neg]),\n",
    "        x='coef', y='feature'\n",
    "    )\n",
    "    plt.title(f\"{cls} — Top Positive/Negative TF-IDF Coefficients\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Word Cloud (Positive Features) ===\n",
    "    text = ' '.join(top_pos['feature'])\n",
    "    wc = WordCloud(\n",
    "        width=400, height=200,\n",
    "        background_color='white',\n",
    "        mask=mask\n",
    "    ).generate(text)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{cls} — Word Cloud (Positive Features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curves (TF-IDF Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "for i, cls in enumerate(classes):\n",
    "    prob_pos = 1 / (1 + np.exp(-y_score[:, i]))  # sigmoid to convert to prob\n",
    "    frac_pos, mean_pred_val = calibration_curve(y_test_bin[:, i], prob_pos, n_bins=10)\n",
    "\n",
    "    plt.plot(mean_pred_val, frac_pos, \"s-\", label=f\"{cls}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Curves per Class\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing TF-IDF Model in 3D\n",
    "\n",
    "This section reduces the high-dimensional TF-IDF features to 3 components using TruncatedSVD, and visualizes:\n",
    "\n",
    "- Class distributions in reduced space\n",
    "- Logistic regression decision surfaces (predicted probabilities)\n",
    "\n",
    ">**Note:** This is a projection of the original feature space. While useful for interpretation, it does **not fully represent** how the classifier behaves in the full-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dimensionality Reduction ===\n",
    "svd = TruncatedSVD(n_components=3, random_state=42)\n",
    "X_vis_3d = svd.fit_transform(X_test_tfidf)\n",
    "\n",
    "# === Train Logistic Regression on 3D projection ===\n",
    "clf_3d = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    max_iter=10000\n",
    ")\n",
    "clf_3d.fit(X_vis_3d, y_test)\n",
    "\n",
    "# === Label encoding for coloring ===\n",
    "label_encoder = LabelEncoder().fit(y_test)\n",
    "y_encoded = label_encoder.transform(y_test)\n",
    "class_names = label_encoder.classes_\n",
    "colors = px.colors.qualitative.Light24\n",
    "\n",
    "# === Plot setup ===\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot points per class\n",
    "for i, class_name in enumerate(class_names):\n",
    "    idx = (y_encoded == i)\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=X_vis_3d[idx, 0], y=X_vis_3d[idx, 1], z=X_vis_3d[idx, 2],\n",
    "        mode='markers',\n",
    "        name=class_name,\n",
    "        marker=dict(size=3, color=colors[i % len(colors)], opacity=0.9)\n",
    "    ))\n",
    "\n",
    "# Create grid for 3D surface\n",
    "x_range = np.linspace(X_vis_3d[:, 0].min(), X_vis_3d[:, 0].max(), 30)\n",
    "y_range = np.linspace(X_vis_3d[:, 1].min(), X_vis_3d[:, 1].max(), 30)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "zz_base = np.zeros_like(xx)\n",
    "grid_flat = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_3d = np.c_[grid_flat, zz_base.ravel()]\n",
    "\n",
    "# Add predicted probability surfaces (1 per class)\n",
    "for i in range(len(class_names)):\n",
    "    probs = clf_3d.predict_proba(grid_3d)[:, i].reshape(xx.shape)\n",
    "    fig.add_trace(go.Surface(\n",
    "        x=xx, y=yy, z=probs,\n",
    "        opacity=0.4,\n",
    "        colorscale='Viridis',\n",
    "        name=f\"Prob Surface: {class_names[i]}\",\n",
    "        showlegend=True,\n",
    "        visible=\"legendonly\",\n",
    "        colorbar=dict(x=0.9)\n",
    "    ))\n",
    "\n",
    "# Layout and export\n",
    "fig.update_layout(\n",
    "    title=\"3D Probability Surfaces (TF-IDF + Logistic Regression)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"SVD Component 1\",\n",
    "        yaxis_title=\"SVD Component 2\",\n",
    "        zaxis_title=\"Predicted Probability\",\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=1, y=1, z=0.8)\n",
    "    ),\n",
    "    legend=dict(\n",
    "        font=dict(size=10),\n",
    "        itemsizing='constant',\n",
    "        itemdoubleclick=\"toggle\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html(\"tfidf_logreg_3d_decision_planes.html\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-Based Classification (XLM-R)\n",
    "\n",
    "This section uses the multilingual **XLM-RoBERTa** model (`xlm-roberta-base`) for fine-tuning on support ticket data.\n",
    "\n",
    "Key steps:\n",
    "- Tokenization and padding using Hugging Face's `AutoTokenizer`\n",
    "- Fine-tuning with `Trainer` and early stopping\n",
    "- Evaluation on the same test split used for the TF-IDF baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Preparing data for transformer model...\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Map labels to integers\n",
    "label2id = {label: i for i, label in enumerate(sorted(y.unique()))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Prepare DataFrames for Hugging Face Datasets\n",
    "df_train = pd.DataFrame({\"text\": X_train, \"label\": y_train.map(label2id)})\n",
    "df_test = pd.DataFrame({\"text\": X_test, \"label\": y_test.map(label2id)})\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Apply tokenization\n",
    "ds_train = ds_train.map(tokenize, batched=True)\n",
    "ds_test = ds_test.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Metric Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with appropriate label mapping\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Metric function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds, average='macro'),\n",
    "        \"accuracy\": accuracy_score(labels, preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    seed=42,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate XLM-R\n",
    "\n",
    "After defining the model and trainer, we now:\n",
    "\n",
    "- Fine-tune the transformer on the training set\n",
    "- Evaluate predictions on the test set\n",
    "- Visualize model performance with a confusion matrix and loss curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + Evaluation + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training transformer model...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nEvaluating transformer model...\")\n",
    "preds = trainer.predict(ds_test)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "# Print classification metrics\n",
    "print(classification_report(\n",
    "    df_test['label'],\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(len(id2label))]\n",
    "))\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(\n",
    "    confusion_matrix(df_test['label'], y_pred),\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Greens'\n",
    ")\n",
    "plt.title(\"XLM-R Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Hugging Face log history to DataFrame\n",
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Extract and align training and validation loss per epoch\n",
    "train_loss_df = log_df[['epoch', 'loss']].dropna().groupby('epoch').last().reset_index()\n",
    "val_loss_df = log_df[['epoch', 'eval_loss']].dropna().groupby('epoch').last().reset_index()\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss_df['epoch'], train_loss_df['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(val_loss_df['epoch'], val_loss_df['eval_loss'], label='Validation Loss', marker='o')\n",
    "plt.title(\"Training and Validation Loss (XLM-R)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Visuals: XLM-R Model\n",
    "\n",
    "The following cells provide deeper insight into the transformer model:\n",
    "\n",
    "1. Embedding projection with t-SNE\n",
    "2. Calibration curves per class\n",
    "3. Most confident misclassifications\n",
    "4. Word clouds per class (based on raw token frequency)\n",
    "\n",
    "> Note: Embedding plots are based on a reduced-dimensional view of transformer output and may not fully represent real model decision boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Projection (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from last hidden state (mean pooled)\n",
    "with torch.no_grad():\n",
    "    enc2 = tokenizer(df_test['text'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "    enc2 = {k: v.to(model.device) for k, v in enc2.items()}\n",
    "    out2 = model.base_model(**enc2)\n",
    "\n",
    "    if hasattr(out2, 'pooler_output') and out2.pooler_output is not None:\n",
    "        embeds = out2.pooler_output.cpu().numpy()\n",
    "    else:\n",
    "        embeds = out2.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Project with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "proj = tsne.fit_transform(embeds)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=proj[:, 0], y=proj[:, 1], hue=df_test['label'].map(id2label), palette='tab10', alpha=0.7)\n",
    "plt.title('t-SNE of XLM-R Embeddings')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class probabilities from logits\n",
    "probs = torch.softmax(torch.tensor(preds.predictions), dim=-1).numpy()\n",
    "\n",
    "# Plot calibration curve per class\n",
    "plt.figure(figsize=(6, 4))\n",
    "for class_id in range(len(label2id)):\n",
    "    true_labels = (df_test['label'] == class_id).astype(int)\n",
    "    prob_true, prob_pred = calibration_curve(true_labels, probs[:, class_id], n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=id2label[class_id])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "plt.title('Calibration Curves per Class')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most confident wrong predictions\n",
    "mis_idx = np.where(y_pred != df_test['label'].values)[0]\n",
    "conf = np.max(probs, axis=1)\n",
    "top_mis = mis_idx[np.argsort(-conf[mis_idx])][:5]\n",
    "\n",
    "# Show in DataFrame\n",
    "mis_df = pd.DataFrame({\n",
    "    'text': df_test['text'].iloc[top_mis].values,\n",
    "    'true': [id2label[i] for i in df_test['label'].iloc[top_mis]],\n",
    "    'pred': [id2label[i] for i in y_pred[top_mis]],\n",
    "    'conf': conf[top_mis]\n",
    "})\n",
    "\n",
    "display(mis_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple word clouds for each predicted class\n",
    "for class_id, label_str in id2label.items():\n",
    "    texts = df_test.loc[df_test['label'] == class_id, 'text']\n",
    "    if texts.empty:\n",
    "        continue\n",
    "\n",
    "    combined = \" \".join(texts.tolist())\n",
    "    wc = WordCloud(width=400, height=200, background_color='white').generate(combined)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Class: {label_str}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
